{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   NATURAL LANGUAGE PROCESSING PROJECT\n",
    "###   Text Data Analysis and Preprocessing\n",
    "-   Overview\n",
    "    -   This project involves Natural Language Processing (NLP). The primary objective is to analyze customer reviews, a rich and insightful source of data, to extract meaningful insights that can drive informed decisions and strategies.\n",
    "\n",
    "-   The Project Scope encompasses several key NLP tasks and techniques:\n",
    "\n",
    "    -   Text Cleaning: We begin by preprocessing the raw review data. This includes removing unnecessary elements such as punctuations, HTML tags, URLs, emojis, and stopwords. This step is crucial for reducing noise and standardizing the text, thereby making it more conducive to analysis.\n",
    "\n",
    "    -   Text Analysis: Through various methods, including word count analysis and stop word frequency analysis, we seek to understand the patterns and trends in the text data. We also examine the occurrence of URLs in reviews, which can signify external references or resources.\n",
    "\n",
    "    -   Data Visualization: Leveraging tools like matplotlib and seaborn, we visualize our findings through graphs and charts. This not only aids in better comprehension of the data but also highlights key patterns and anomalies that might warrant further investigation.\n",
    "\n",
    "    -   Lemmatization: To capture the essence of words more effectively, we implement lemmatization, which reduces words to their base or dictionary form. Unlike stemming, lemmatization takes into account the context of words, ensuring a more accurate and meaningful representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Import neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings\n",
    "plt.style.use('ggplot')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Load the trustpilot reviews dataset for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset\n",
    "file_path = \"C:/Users/Oby/Desktop/Data Science Portfolio/trust_pilot_reviews_data_2022_06.csv\"\n",
    "data= pd.read_csv(file_path)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Understanding the Data's Size, Shape, and Structure to gain a fundamental understanding of the dataset, explore its size, shape, and overall structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's get a fundamental understanding of the data's size, shape, and structure\n",
    "# Obtain the shape of the data\n",
    "data.info()\n",
    "print(data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Clean up the dataset by removing columns that are not necessary for the analysis to simplify the dataset, making it easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted columns from the original dataset\n",
    "data.drop(['company_url', 'trustpilot_url', 'description', 'author_name', 'reviewed_at', 'name', 'scraped_at', 'uniq_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-    Check for rmissing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing values in the dataset\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Distribution of Ratings\n",
    "-   Use the value_counts() method on the 'rating' column to understand the distribution of customer ratings in our dataset. This method provides a count of the number of occurrences of each unique value in the column, which in this case corresponds to the different ratings given by customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the value counts of the 'rating' column\n",
    "rating_counts = data['rating'].value_counts()\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "rating_counts.plot(kind='bar', color='green')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Distribution of Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Make this a binary classification problem where ratng from 1 to 3 are classified bad and rating 4 to 5 are good "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'sentiment' based on the rating values\n",
    "data['sentiment'] = data['rating'].apply(lambda x: 'Bad' if x <= 3 else 'Good')\n",
    "\n",
    "# Now, 'data' includes a 'sentiment' column with \"Bad\" or \"Good\" labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Sentiment Distribution\n",
    "-   Visualize the distribution of different sentiments in the dataset. Understanding the sentiment distribution is crucial as it provides insights into the overall sentiment trends in the data, such as whether positive, negative, or neutral sentiments are more prevalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each sentiment\n",
    "sentiment_counts = data['sentiment'].value_counts()\n",
    "\n",
    "# Create a pie chart\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Add a title\n",
    "plt.title('Sentiment Distribution')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REVEIEW TEXT ANALYSIS\n",
    "-   Data exploration and visualisation: Explore the dataset and understand the context behind each sentiment category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Group the dataset by the 'sentiment' column and then display a select number of review texts from each group. This approach helps to get a qualitative feel for the data, providing insights into the nature and tone of reviews associated with different sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataset by \"sentiment\"\n",
    "grouped = data.groupby('sentiment')\n",
    "\n",
    "# Define the number of reviews to display for each sentiment\n",
    "reviews_to_display = 5\n",
    "\n",
    "# Display the first 5 review texts\n",
    "for sentiment, group in grouped:\n",
    "    print(f\"Sentiment: {sentiment}\\n\")\n",
    "    for i, review_text in enumerate(group['review_text']):\n",
    "        if i >= reviews_to_display:\n",
    "            break  # Stop after displaying the first 10 reviews\n",
    "        print(review_text)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the average words in the review text\n",
    "totalreviews = list(data['review_text'])\n",
    "length = []\n",
    "for i in range(0,len(totalreviews)):\n",
    "        totalreviews[i] = str(totalreviews[i])\n",
    "        a = len(totalreviews[i].split(' '))\n",
    "        length.append(a)\n",
    "\n",
    "    \n",
    "print(\"On average a review has about:\", sum(length)/len(length),\"words in them\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the average words in the review title\n",
    "totalreviews = list(data['review_title'])\n",
    "length = []\n",
    "for i in range(0,len(totalreviews)):\n",
    "        totalreviews[i] = str(totalreviews[i])\n",
    "        a = len(totalreviews[i].split(' '))\n",
    "        length.append(a)\n",
    "\n",
    "    \n",
    "print(\"On average a review title has about:\", sum(length)/len(length),\"words in them\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -  Visualise the variation in the length of reviews across different sentiment categories. Analyzing the number of words per review can provide insights into whether certain sentiments are associated with longer or shorter reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['words per review'] = data['review_text'].str.split().apply(len)\n",
    "data.boxplot('words per review', by= 'sentiment', grid=False, showfliers=False, color='Blue')\n",
    "plt.suptitle('')\n",
    "plt.xlabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count Analysis for Review Text\n",
    "-   Analyze the word count in review texts and compare how they vary between 'Good' and 'Bad' sentiments. This analysis can reveal whether there is a tendency for reviews with a particular sentiment to be longer or shorter in terms of word count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#WORD COUNT ANALYSIS FOR REVIEW TEXT\n",
    "# Function to count words in a text sample\n",
    "def count_words(str):\n",
    "    words = str.split()\n",
    "    return len(words)\n",
    "\n",
    "def plot_count(count_ones,count_zeros,title_1,title_2,subtitle):\n",
    "    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n",
    "    sns.distplot(count_zeros,ax=ax1,color='Blue')\n",
    "    ax1.set_title(title_1)\n",
    "    sns.distplot(count_ones,ax=ax2,color='Red')\n",
    "    ax2.set_title(title_2)\n",
    "    fig.suptitle(subtitle)\n",
    "    plt.show()\n",
    "\n",
    "# Calculate word counts for 'good' sentiment\n",
    "good_sentiment_data = data[data['sentiment'] == 'Good']\n",
    "good_sentiment_data['word_count'] = good_sentiment_data['review_text'].apply(count_words)\n",
    "\n",
    "\n",
    "# Calculate word counts for 'bad' sentiment\n",
    "bad_sentiment_data = data[data['sentiment'] == 'Bad']\n",
    "bad_sentiment_data['word_count'] = bad_sentiment_data['review_text'].apply(count_words)\n",
    "\n",
    "\n",
    "# Create a line chart for word counts with curves using the plot_count function\n",
    "plot_count(good_sentiment_data['word_count'], bad_sentiment_data['word_count'], 'Good Sentiment', 'Bad Sentiment', 'Word Count Analysis For Review Text')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Word Count Analysis in Review Text\n",
    "-   Analyse the presence of stop words in review texts and how they vary between 'Good' and 'Bad' sentiments. Stop words are commonly used words in a language (like \"the\", \"is\", \"in\", etc.) that are often filtered out in NLP tasks due to their low informational content. However, the frequency of stop words can sometimes provide insights into the writing style or the nature of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to count stop words in a text sample\n",
    "def count_stop_words(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = data.split()\n",
    "    return sum(1 for word in words if word.lower() in stop_words)\n",
    "\n",
    "# Calculate word counts for 'good' sentiment\n",
    "good_sentiment_data['stop_word_count'] = good_sentiment_data['review_text'].apply(count_stop_words)\n",
    "\n",
    "# Calculate word counts for 'bad' sentiment\n",
    "bad_sentiment_data['stop_word_count'] = bad_sentiment_data['review_text'].apply(count_stop_words)\n",
    "\n",
    "# Create a plot using the plot_count function\n",
    "plot_count(good_sentiment_data['stop_word_count'], bad_sentiment_data['stop_word_count'], 'Good Sentiment', 'Bad Sentiment', 'Stop Word Count Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMMON STOP WORDS IN THE REVIEW TEXT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to count stop words\n",
    "def count_stop_words(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = data.split()\n",
    "    return sum(1 for word in words if word.lower() in stop_words)\n",
    "\n",
    "# Get the most common stop words (top 50) for 'good' sentiment\n",
    "top_stop_words_good = Counter(\" \".join(good_sentiment_data['review_text']).split()).most_common(50)\n",
    "\n",
    "# Get the most common stop words (top 50) for 'bad' sentiment\n",
    "top_stop_words_bad = Counter(\" \".join(bad_sentiment_data['review_text']).split()).most_common(50)\n",
    "\n",
    "# Extract words and counts for plotting\n",
    "x_good, y_good = zip(*top_stop_words_good)\n",
    "x_bad, y_bad = zip(*top_stop_words_bad)\n",
    "\n",
    "# Create bar charts\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.barh(x_good, y_good, color='green')\n",
    "plt.title(\"Most Common Stop Words in 'Good' Sentiment\")\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Words')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.barh(x_bad, y_bad, color='red')\n",
    "plt.title(\"Most Common Stop Words in 'Bad' Sentiment\")\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Words')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Analyze the occurrence of URLs in review texts and compare their average counts between reviews with 'Good' and 'Bad' sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL ANALYSIS IN REVIEW TEXT\n",
    "# Function to count URLs in a text sample\n",
    "def count_urls(data):\n",
    "    return len([x for x in str(data).lower().split() if 'http' in x or 'https' in x])\n",
    "\n",
    "# Calculate urlcounts for 'good' sentiment\n",
    "good_sentiment_data['count_good_urls'] = good_sentiment_data['review_text'].apply(count_urls)\n",
    "\n",
    "# Calculate url  counts for 'bad' sentiment\n",
    "bad_sentiment_data['count_bad_urls'] = bad_sentiment_data['review_text'].apply(count_urls)\n",
    "\n",
    "# Bar Chart for Average URL Counts\n",
    "avg_good_urls = good_sentiment_data['count_good_urls'].mean()\n",
    "avg_bad_urls = bad_sentiment_data['count_bad_urls'].mean()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(['Good Sentiment', 'Bad Sentiment'], [avg_good_urls, avg_bad_urls], color=['blue', 'red'])\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Average URL Count')\n",
    "plt.title('Average URL Counts in Review Texts by Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REVIEW TITLE ANALYSIS\n",
    "-   Explore the review titles grouped by sentiment to understand the themes and expressions commonly used in the titles of reviews with different sentiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataset by \"sentiment\"\n",
    "grouped = data.groupby('sentiment')\n",
    "\n",
    "# Define the number of reviews to display for each sentiment\n",
    "reviews_to_display = 10\n",
    "\n",
    "# Display the first 10 review texts\n",
    "for sentiment, group in grouped:\n",
    "    print(f\"Sentiment: {sentiment}\\n\")\n",
    "    for i, review_text in enumerate(group['review_title']):\n",
    "        if i >= reviews_to_display:\n",
    "            break  # Stop after displaying the first 10 reviews\n",
    "        print(review_text)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count Analysis for Review title\n",
    "-   Analyze the word count in review titles and compare how they vary between 'Good' and 'Bad' sentiments. This analysis can reveal whether there is a tendency for tiltes with a particular sentiment to be longer or shorter in terms of word count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count words in a text sample\n",
    "def count_words(data):\n",
    "    words = data.split()\n",
    "    return len(words)\n",
    "\n",
    "# Calculate word counts for 'good' sentiment\n",
    "good_sentiment_data = data[data['sentiment'] == 'Good']\n",
    "good_sentiment_data['word_count'] = good_sentiment_data['review_title'].apply(count_words)\n",
    "\n",
    "# Calculate word counts for 'bad' sentiment\n",
    "bad_sentiment_data = data[data['sentiment'] == 'Bad']\n",
    "bad_sentiment_data['word_count'] = bad_sentiment_data['review_title'].apply(count_words)\n",
    "\n",
    "# Create a plot using the plot_count function\n",
    "plot_count(good_sentiment_data['word_count'], bad_sentiment_data['word_count'], 'Good review', 'Bad review', 'Review Title Word Count Analysis')\n",
    "#plot_count(good_sentiment_data['word_count'], bad_sentiment_data['word_count'],  'Word Count Analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common stop words in the review title \n",
    "-   Analyse the presence of stop words in review titles and how they vary between 'Good' and 'Bad' sentiments. Stop words are commonly used words in a language (like \"the\", \"is\", \"in\", etc.) that are often filtered out in NLP tasks due to their low informational content. However, the frequency of stop words can sometimes provide insights into the writing style or the nature of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count stop words in a text sample\n",
    "def count_stop_words(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = data.split()\n",
    "    return sum(1 for word in words if word.lower() in stop_words)\n",
    "\n",
    "# Calculate word counts for 'good' sentiment\n",
    "good_sentiment_data['stop_word_count'] = good_sentiment_data['review_title'].apply(count_stop_words)\n",
    "\n",
    "# Calculate word counts for 'bad' sentiment\n",
    "bad_sentiment_data['stop_word_count'] = bad_sentiment_data['review_title'].apply(count_stop_words)\n",
    "\n",
    "# Create a plot using the plot_count function\n",
    "plot_count(good_sentiment_data['stop_word_count'], bad_sentiment_data['stop_word_count'], 'Good review title', 'Bad review title', 'Stop Word Count Analysis in Review Title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to count stop words\n",
    "def count_stop_words(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = data.split()\n",
    "    return sum(1 for word in words if word.lower() in stop_words)\n",
    "\n",
    "# Get the most common stop words (top 50) for 'good' sentiment\n",
    "top_stop_words_good = Counter(\" \".join(good_sentiment_data['review_title']).split()).most_common(50)\n",
    "\n",
    "# Get the most common stop words (top 50) for 'bad' sentiment\n",
    "top_stop_words_bad = Counter(\" \".join(bad_sentiment_data['review_title']).split()).most_common(50)\n",
    "\n",
    "# Extract words and counts for plotting\n",
    "x_good, y_good = zip(*top_stop_words_good)\n",
    "x_bad, y_bad = zip(*top_stop_words_bad)\n",
    "\n",
    "# Create bar charts\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.barh(x_good, y_good, color='green')\n",
    "plt.title(\"Most Common Stop Words in 'Good' Review Title\")\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Words')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.barh(x_bad, y_bad, color='red')\n",
    "plt.title(\"Most Common Stop Words in 'Bad' Review Title\")\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Words')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data\n",
    "-   Text Cleaning for Review Data\n",
    "    -   Preprocess the text data in the 'review_title' and 'review_text' columns of our dataset. Text cleaning is an essential step in data preprocessing for Natural Language Processing (NLP), as it helps in removing noise and irrelevant content, thereby making the text data more uniform and analyzable.\n",
    "\n",
    "-   The text cleaning involves several steps:\n",
    "    -   Defining the Text Cleaning Function:\n",
    "\n",
    "-   Removing Punctuations and HTML Tags: Use regular expressions to remove punctuations and HTML syntaxes. This makes the text cleaner and more consistent.\n",
    "    -   Removing URLs and Emojis: We also remove URLs and emojis from the text, as they are often not necessary for typical text analysis tasks.\n",
    "    -   Filtering Out Stop Words: Stop words (common words that usually don't carry much meaning, like 'the', 'is', etc.) are removed. This step focuses the analysis on more meaningful words in the text.\n",
    "    -   Converting to Lowercase: The text is converted to lowercase to ensure uniformity, as 'Word' and 'word' should be considered the same word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove punctuations, HTML syntaxes, URLs, Emojis, and stopwords\n",
    "    text = re.sub(r'&', 'and', text)  # Replace '&' with 'and'\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove punctuations\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML syntaxes\n",
    "    text = ' '.join(word for word in text.split() if not (word.startswith(\"http://\") or word.startswith(\"https://\")))  # Remove URLs\n",
    "\n",
    "    # Remove Emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = ' '.join(filtered_words).lower()\n",
    "    return text\n",
    "\n",
    "# Apply the clean_text function to both 'review_title' and 'review_text' columns\n",
    "data['review_title'] = data['review_title'].apply(clean_text)\n",
    "data['review_text'] = data['review_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization of Review Texts and review titles\n",
    "-   Lemmatization is a crucial step in the preprocessing of text data for Natural Language Processing (NLP) tasks. It involves reducing words to their base or root form. Unlike stemming, lemmatization considers the context and converts the word to its meaningful base form, which is known as the lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the  lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create copies of the data for stemming and lemmatization\n",
    "lemmatization_data = data.copy()\n",
    "\n",
    "# Function for lemmatization\n",
    "def lemmatize_text(text):\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply lemmatization to the other copy of the data\n",
    "lemmatization_data['review_title'] = lemmatization_data['review_title'].apply(lemmatize_text)\n",
    "lemmatization_data['review_text'] = lemmatization_data['review_text'].apply(lemmatize_text)\n",
    "\n",
    "#Display the first 5 rows of the lematized data\n",
    "lemmatization_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting Lemmatized Data to a CSV File\n",
    "After completing the lemmatization of the review texts, export this processed data to a CSV file. This step is crucial for preserving the lemmatized data, making it easily accessible for future analysis or use in other applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatization_data.to_csv('lemmatized_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
